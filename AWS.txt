Encrypt unencrypted EBS volume / RDS database

* create a snapshot, create new encrypted EBS volume from it
* copy it into an encrypted one, create new volume/db from it

Capacity Reservations

Amazon EC2 on-demand Capacity Reservations enable you to create and manage
reserved capacity on Amazon EC2 without any long-term commitment or fixed
terms. This can be very beneficial if you regularly face
InsufficientInstanceCapacity errors when AWS doesn't have enough available
on-demand capacity while starting or launching an EC2 instance.

For cluster placement groups, capacity also means running on the 'same'
hardware and that hardware needs to be reserved.
Use Reserved Instances instead if you want to commit to 1 or 3 years

Placement groups

  cluster -  low latency,  low availability: share hardware => same EC2 type
partition -  avg latency,  avg availability: spread partitions across hardware, 7 partitions per AZ
   spread - high latency, high availability: multi AZ, 7 instances in different racks in 1 group per AZ, diff hw allows for different EC2 types

Elastic Load Balancer

ALB: distribute load based on    http/https + port, headers, ...
NLB: distribute load based on TCP, UDP, TLS + port
     define static IPs (per AZ) for the NLB (so customers can whitelist us)

* dispatch requests to target groups based on routes, query string, headers, IPs (ALB)
                                           /url1 -> tg1
                          /url2, one.example.com -> tg2
  two.example.com, ?platform=mobile, HTTP header -> tg3

  then spread load across multiple or single (e.g containers) host(s) in the
  target group based on health checks (port + route) => seamless handling of
  downstream instances failures

* SSL termination

IAM principal = user or role

Service-Linked Role:
only a specific service can use this role vs a regular role which can be used by all services/users

  SG - statefull, if one way is allowed then the return way is automatically allowed
NACL - stateless, both ways are always evaluated

SG
EC2, ELB, EFS, RDS, ElastiCache

A private host behind NAT "can't" be contacted, for that you need NAT traversal:
Also known as UDP encapsulation, it allows traffic to get to the specified
destination which doesn't have a public IP address. In a S2S VPN connection, a
CGW behind NAT needs this enabled

CloudFront

CloudFront price classes:
class all - all regions
class 200 - all, without the most expensive ones
class 100 - least expensive regions only

origins

multi-origins based on /path/* + origin groups (primary/secondary) for failover

CloudFront (CDN - cache content at the edge)
S3 or any http/https/rtmp endpoint as origin

Global Accelerator (use edge locations to accelerate APPS (no S3), no caching)
UDP, IoT (MQTT), VOIP endpoint
           http/https endpoint (if static IPs needed)

field level encryption:
extra security on top of https - specify up to 10 fields in your POST request
asymmetric encryption (e.g credit card details) on the edge, decrypted by app (e.g behind ALB origin) private key

Global Accelerator

It's a global load balancer!
region endpoint groups (akin to TG) -> endpoints (e.g EC2 but can also be ELBs) with health checks

A persistent spot request is like an ASG. It will keep spawning instances till the end of its validity period.

1 CPU = multiple cores = multiple threads. vCPU is the total of threads.

EBS
                            max
gp3 | 1 GiB to 16 TiB |  16 000  iops | 1000 MiB/s | not multi-attach
io2 | 4 GiB to 16 TiB |  32 000 piops |            |
    |                 |  64 000 piops |            | if Nitro
    | 4 GiB to 64 TiB | 256 000 piops |            | if block express

shared storage:
EC2 <-> ENI -> EFS (NFS),
            -> FSx for Lustre (Linux cluster)
            -> FSx for Windows (SMB, NTFS)

FSx persistent file system: data is replicated
FSx scratch    file system: temp storage, faster, cheaper

hybrid storage integration (storage gateways needed because S3 proprietary):
on-premises                                                    | cloud
---------------------------------------------------------------+--------------------------------------
 app server,     file gateway (NFS, SMB), IAM, optional AD auth  | S3, S3 IA (both) -> S3 IA, S3 glacier
             FSx file gateway (SMB, NTFS, AD)                    | FSx for Windows file server
 app server,   volume gateway (iSCSI)                            | S3               -> S3 EBS snapshots
data server,     tape gateway (iSCSI, VTL (virtual tape library) | S3               -> S3 glacier

volume gateway specific
cached volumes: main data is on S3 with local on-prem cache for fast access
stored volumes: main data is on-prem with async backup to S3

virtualization is needed (???) to install the gateways, instead we can buy a HW appliance

ASG

Use golden AMI so updates, app install, ... take less time => we can use a smaller cooldown period

In AWS there is a network cost when data moves between AZs,
but not for RDS read replicas within the same region (only cross-region)

Aurora DB
writer + reader OR custom endpoint

ElastiCache
* requires heavy code changes
* no IAM auth, redis auth or Memcached SASL
redis: sorted sets for real-time leaderboards

Route 53

Alias: CNAME to 1 managed AWS resource (no EC2!)
       no TTL, can point to zone apex, free

record with multiple A values -> the client will choose at random (client side LB)

health checks: only return IPs for healthy resources
               e.g give me a healthy ALB, then target group health check to give me a healthy EC2 instance

routing policies:

- simple (no health checks)
- weighted
  weight.example.com 70 - 7.8.9.1
  weight.example.com 30 - 3.4.5.6
  weight.example.com 10 - 1.1.8.8
- latency
- failover (primary active / secondary passive)
- geolocation (default IP mandatory)
- geoproximity (traffic flow, bias -1 .. 99)
- multi-value (again client side LB but with health checks, return up to 8 IPs)

GoDaddy registrar with Route 53 DNS:
register domain with GoDaddy but specify custom nameservers (AWS ones) where the records will be defined

http statefullness can be achieved with:
* ELB stickiness (session/client affinity)
* cookies stored on EC2 instances or sent by user (web cookies)
* single session_id cookie sent by client, session info stored in ElastiCache

Beanstalk
dev centric view, infrastructure is transparent
PaaS: versioned application / environment (dev,test,prod) +    web tier (ELB -> ASG) or
                                                            worker tier (SQS <- ASG)

S3 (web URL - http/https)

3_500 PUT req/s per prefix
5_500 GET req/s per prefix, both limited by KMS (5_500, 10_000, 30_000 req/s based on region, increase with quotas)

naming: 3-63 -> no upper, _, IP; start with [a-z0-9]
        s3://bucket-name/folder-1/folder-2/my-image.jgp > max 5TB, multi-part upload if >5GB
                         prefix          + name = key

with versioning enabled, removal of an object adds a 'delete marker'.
deleting a specific version or a 'delete marker' one is permanent.

storage classes:
        std, std-ia -> min 30 days
intelligent tiering -> small monthly monitoring and auto-tiering fee
     amazon glacier -> archives go in vaults
                       90 days min, 180 for deep archive
retrieval cost per GB for all but std/intelligent

lifecycle rules
- transition actions
- expiration actions (deletion)

replication isn't chained:
A -> B -> C doesn't mean A -> C.
objects in B replicated from A aren't considered new. only explicit new ones will be replicated to C

encryption:
SSE-S3  = "x-amz-server-side-encryption": "AES256",  in header
SSE-KMS = "x-amz-server-side-encryption": "aws:kms", in header
SSE-C   =                                       key, in header (https mandatory)
CSE     = client side encryption

the default encryption setting will be applied only to non-encrypted objects,
meaning that if an object is already encrypted (e.g via bucket policy) it won't be altered.

block all public access
* to buckets/objects                 via new ACLs
* to buckets/objects                 via ANY ACLs (existing ones too)
* to buckets/objects                 via new public bucket or access point policies
* to buckets/objects + cross-account via ANY public bucket or access point policies

pre-signed URLs
generate GET ones with cli, GET/PUT ones with SDK (creator's get/put permissions inherited by users)
valid for a limited time only (3600s by default)

note: with CloudFront in front of our bucket, we must use CloudFront signed
URLs for single files or signed cookies for multiple files because bucket
access is restricted to the OAI.

CORS: cross-origin resource sharing (web browser security mechanism)
get index.html                         from www.example.com (origin - protocol://domain:port),
    index.html tries to get a resource from   net.games.com (cross-origin)
                                              net.games.com needs to send headers Access-Control-Allow-Origin:  https://www.example.com
                                                                                  Access-Control-Allow-Methods: GET, ...

Snow family (EBS and S3 storage)

             snowcone (  8TB)             2  CPU,   4GiB RAM, no battery/cables, can use DataSync once online
snowball edge compute ( 42TB) optimized: 52 vCPU, 208GiB RAM, optional GPU
snowball edge storage ( 80TB) optimized: 40 vCPU,  80GiB RAM, up to 15 nodes storage cluster, cannot import to glacier directly
           snowmobile (100PB) - prefer to snowball if >10PB

OpsHub: AWS 'snow' Console on your laptop

Async app to app communication

                         queue model: SQS (256kb per msg, 4 to 14 days retention) <- poll for up to 10 messages
                       pub/sub model: SNS
real-time streaming (~pub/sub) model: kinesis data streams (records with partition key, same key goes to same shard => ordering can be achieved)
                                      |
                           producers  v  consumers
             kinesis agent, SDK, KPL  ‒  SDK, KCL
              1 MB/s (or 1000 msg/s)  ‒  2 MB/s per shard per all      (shared)
                           per shard  ‒  2 MB/s per shard per consumer (enhanced fanout)

             kinesis data firehose: batch writes (near real time) - 32MB or 60s
                                    custom data transform with lambda
                                    => S3, redshift (via S3), elasticsearch

SQS
visibility timeout (30s): message invisible to other consumers,
ChangeMessageVisibility API call if not done processing

MaximumReceives: times a msg is allowed to go back to the queue,
                 then move it to DLQ (dead letter queue)

Delay queue: hide message for up to 15min, send with DelaySeconds can override this

long polling (up to 20s) => less API calls. enable at Q level or WaitTimeSeconds API

request-response systems:
the producers (requesters) send messages ( [ID/response Q (answer expected there)] ) to a single request Q,
the consumers (responders) reply via many virtual Qs (SQS Temporary Queue Java Client needed)

ordering:
FIFO + group ID: block group A messages for other consumers while a group A
                 batch is in flight (being processed, eg. A3-A2-A1), else a
                 consumer could process say A4 before and the ordering would be broken => A3-A2-A1-A4

SNS

           topic,
publish to phone (SMS),
           platform endpoint (e.g ADM: Amazon device messaging)

100_000 topics -> 12_500_000 subscriptions per topic (optional JSON policy to filter messages)

FIFO topic: ordering of messages per group
            subscribers can only be SQS FIFO (throughput limited to 300/s)

CloudWatch

while exploring metrics in the console, I couldn't filter for sqs, it only worked by filtering for aws first! bug (???)

Elastic Container Service

ECS cluster
   container instances (e.g EC2)
      services - our app will be a versioned service (v1, v2, ...)
          tasks - tasks are isolated in services, many services can be defined on the same container instances.

launch types
*     EC2: 1 ECS agent        per instance, the agent will use the EC2 instance profile role <-> ECS, ECR, CloudWatch
* Fargate: 1 ENI (private IP) per task,     the task  will use  an ECS task role

share data among tasks by mounting EFS volumes onto the tasks

ALB - TG:
individual processes run on separate EC2 instances =>
if ALB listens on port 80, the process can also listen on port 80

ALB - service on EC2:
multiple tasks can reside on the same container instance => if ALB listens on
port 80, all tasks can't listen on port 80 so they listen on random ports which
the ALB will automatically find, therefore on the container instance's SG we
must allow all ports from ALB's SG

ALB - service on Fargate:
on the ENIs SGs allow the task port from the ALB SG

scaling

CloudWatch alarm (e.g on CPU service usage) -> service auto scale -> [for EC2 launch type we would also need an ASG for the container instances]

Events

EventBridge (old CloudWatch events):
* can intercept any AWS events and define action targets for them.
* define CRON jobs (execute task with lambda)
