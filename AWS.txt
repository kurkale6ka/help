Encrypt unencrypted EBS volume / RDS database

* create a snapshot, create new encrypted EBS volume from it
* copy it into an encrypted one, create new volume/db from it

Capacity Reservations

Amazon EC2 on-demand Capacity Reservations enable you to create and manage
reserved capacity on Amazon EC2 without any long-term commitment or fixed
terms. This can be very beneficial if you regularly face
InsufficientInstanceCapacity errors when AWS doesn't have enough available
on-demand capacity while starting or launching an EC2 instance.

For cluster placement groups, capacity also means running on the 'same'
hardware and that hardware needs to be reserved.
Use Reserved Instances instead if you want to commit to 1 or 3 years

Placement groups

  cluster -  low latency,  low availability: share hardware => same EC2 type                                                                     <= HPC
partition -  avg latency,  avg availability: spread partitions across hardware, 7 partitions per AZ                                              <= Big Data
   spread - high latency, high availability: multi AZ, 7 instances in different racks in 1 group per AZ, diff hw allows for different EC2 types  <= HA

Elastic Load Balancer

ALB
* distribute load based on http/https/websocket + port, headers, ...
* connection/SSL termination

dispatch requests to target groups based on routes, query string, headers, IPs (ALB)
                                         /url1 -> tg1
                        /url2, one.example.com -> tg2
two.example.com, ?platform=mobile, HTTP header -> tg3

then spread load across multiple or single (e.g containers) host(s) in the
target group based on health checks (port + route) => seamless handling of
downstream instances failures

NLB
* distribute load based on TCP, UDP, TLS + port
* pass through, request IP goes all the way to the app
* static IPs per AZ => customers can whitelist us

  SG - statefull, if one way is allowed then the return way is automatically allowed
NACL - stateless, both ways are always evaluated

SG
EC2, ELB, EFS, RDS, ElastiCache

CloudFront

CloudFront price classes:
class all - all regions
class 200 - all, without the most expensive ones
class 100 - least expensive regions only

origins

multi-origins based on /path/* + origin groups (primary/secondary) for failover

CloudFront (CDN - cache content at the edge)
S3 or any http/https/rtmp endpoint as origin
                     +- real-time messaging protocol

Global Accelerator (use edge locations to accelerate APPS (no S3), no caching)
UDP, IoT (MQTT), VOIP endpoint
           http/https endpoint (if static IPs needed)

field level encryption:
extra security on top of https - specify up to 10 fields in your POST request
asymmetric encryption (e.g credit card details) on the edge, decrypted by app (e.g behind ALB origin) private key

Global Accelerator

It's a global load balancer!
region endpoint groups (akin to TG) -> endpoints (e.g EC2 but can also be ELBs) with health checks

A persistent spot request is like an ASG. It will keep spawning instances till the end of its validity period.

1 CPU = multiple cores = multiple threads. vCPU is the total of threads.

EBS
                            max
gp3 | 1 GiB to 16 TiB |  16 000  iops | 1000 MiB/s | not multi-attach
io2 | 4 GiB to 16 TiB |  32 000 piops |            |
    |                 |  64 000 piops |            | if Nitro
    | 4 GiB to 64 TiB | 256 000 piops |            | if block express

shared storage:
EC2 <-> ENI -> EFS (NFS),
            -> FSx for Lustre (Linux cluster)
            -> FSx for Windows (SMB, NTFS)

FSx persistent file system: data is replicated
FSx scratch    file system: temp storage, faster, cheaper

hybrid storage integration (storage gateways needed because S3 proprietary):
on-premises                                                      | cloud
-----------------------------------------------------------------+--------------------------------------
 app server,     file gateway (NFS, SMB), IAM, optional AD auth  | S3, S3 IA (both) -> S3 IA, S3 glacier
             FSx file gateway (SMB, NTFS, AD)                    | FSx for Windows file server
 app server,   volume gateway (iSCSI)                            | S3               -> S3 EBS snapshots
data server,     tape gateway (iSCSI, VTL (virtual tape library) | S3               -> S3 glacier

volume gateway specific
cached volumes: main data is on S3 with local on-prem cache for fast access
stored volumes: main data is on-prem with async backup to S3

virtualization is needed (???) to install the gateways, instead we can buy a HW appliance

ASG

EC2 status check (ok or impaired)
* instance status check
* system status check

after scale in/out activites ASG enters the HealthCheckGracePeriod, allowing instances to fully activate before launching/terminating more

Use golden AMI so updates, app install, ... take less time => we can use a smaller HealthCheckGracePeriod aka cooldown period

In AWS there is a network cost when data moves between AZs,
but not for RDS read replicas within the same region (only cross-region)

Aurora DB
auto scaling storage (10GB - 128TB, redshift: 1-128 nodes each up to 128TB)
writer + reader OR custom endpoint

Redshift
* no multi-AZ; better enable automated cluster snapshots cross-region COPY (every 8h, 5GB or on a schedule) for DR.
* spectrum: perform queries directly against S3 (no need to load)
* enhanced VPC routing: stay within VPC, no public Internet

ElastiCache
* heavy code changes required
* no IAM auth, redis auth or Memcached SASL
redis: sorted sets for real-time leaderboards

Route 53

Alias: CNAME to 1 managed AWS resource (no EC2!)
       no TTL, can point to zone apex, free

record with multiple A values -> the client will choose at random (client side LB)

health checks: only return IPs for healthy resources
               e.g give me a healthy ALB, then target group health check to give me a healthy EC2 instance

routing policies:

- simple (no health checks)
- weighted
  weight.example.com 70 - 7.8.9.1
  weight.example.com 30 - 3.4.5.6
  weight.example.com 10 - 1.1.8.8
- latency
- failover (primary active / secondary passive)
- geolocation (default IP mandatory)
- geoproximity (traffic flow, bias -1 .. 99)
- multi-value (again client side LB but with health checks, return up to 8 IPs)

GoDaddy registrar with Route 53 DNS:
register domain with GoDaddy but specify custom nameservers (AWS ones) where the records will be defined

http statefullness can be achieved with:
* ELB stickiness (session/client affinity)
* cookies stored on EC2 instances or sent by user (web cookies)
* single session_id cookie sent by client, session info stored in ElastiCache

Beanstalk
dev centric view, infrastructure is transparent
PaaS: versioned application / environment (dev,test,prod) +    web tier (ELB -> ASG) or
                                                            worker tier (SQS <- ASG)

S3 (web URL - http/https)

3_500 PUT req/s per prefix
5_500 GET req/s per prefix, both limited by KMS (5_500, 10_000, 30_000 req/s based on region, increase with quotas)

naming: 3-63 -> no upper, _, IP; start with [a-z0-9]
        s3://bucket-name/folder-1/folder-2/my-image.jgp > max 5TB, multi-part upload if >5GB
                         prefix          + name = key

with versioning enabled, removal of an object adds a 'delete marker'.
deleting a specific version or a 'delete marker' one is permanent.

storage classes:
        std, std-ia -> min 30 days
intelligent tiering -> small monthly monitoring and auto-tiering fee
     amazon glacier -> archives go in vaults
                       90 days min, 180 for deep archive
retrieval cost per GB for all but std/intelligent

lifecycle rules
- transition actions
- expiration actions (deletion)

replication isn't chained:
A -> B -> C doesn't mean A -> C.
objects in B replicated from A aren't considered new. only explicit new ones will be replicated to C

encryption:
SSE-S3  = "x-amz-server-side-encryption": "AES256",  in header
SSE-KMS = "x-amz-server-side-encryption": "aws:kms", in header
SSE-C   =                                       key, in header (https mandatory)
        => CloudHSM (hardware security module, must use client software)
           * single-tenant, multi-AZ
           * FIPS 140-2 Level 3 (Federal Information Processing Standard)
           * MFA + access & authentication management (users & keys) vs IAM
           * hardware acceleration
           * supported by Redshift
CSE     = client side encryption (could use the Amazon S3 Encryption Client)

the default encryption setting will be applied only to non-encrypted objects,
meaning that if an object is already encrypted (e.g via bucket policy) it won't be altered.

block all public access
* to buckets/objects                 via new ACLs
* to buckets/objects                 via ANY ACLs (existing ones too)
* to buckets/objects                 via new public bucket or access point policies
* to buckets/objects + cross-account via ANY public bucket or access point policies

pre-signed URLs
generate GET ones with cli, GET/PUT ones with SDK (creator's get/put permissions inherited by users)
valid for a limited time only (3600s by default)

note: with CloudFront in front of our bucket, we must use CloudFront signed
URLs for single files or signed cookies for multiple files because bucket
access is restricted to the OAI.

CORS: cross-origin resource sharing (web browser security mechanism)
get index.html                         from www.example.com (origin - protocol://domain:port),
    index.html tries to get a resource from   net.games.com (cross-origin)
                                              net.games.com needs to send headers Access-Control-Allow-Origin:  https://www.example.com
                                                                                  Access-Control-Allow-Methods: GET, ...

MPP: Massively Parallel Processing - athena, redshift (both use Presto(distributed SQL query engine))

Snow family (EBS and S3 storage)

             snowcone (  8TB)             2  CPU,   4GiB RAM, no battery/cables, can use DataSync once online
snowball edge compute ( 42TB) optimized: 52 vCPU, 208GiB RAM, optional GPU
snowball edge storage ( 80TB) optimized: 40 vCPU,  80GiB RAM, up to 15 nodes storage cluster, cannot import to glacier directly
           snowmobile (100PB) - prefer to snowball if >10PB

OpsHub: AWS 'snow' Console on your laptop

Async app to app communication

                         queue model: SQS (256kb per msg, 4 to 14 days retention) <- poll for up to 10 messages
                       pub/sub model: SNS
real-time streaming (~pub/sub) model: kinesis data streams (records with partition key, same key goes to same shard => ordering can be achieved)
                                      |
                           producers  v  consumers
             kinesis agent, SDK, KPL  ‒  SDK, KCL (=> EC2, lambda, ...)
              1 MB/s (or 1000 msg/s)  ‒  2 MB/s per shard per all      (shared)
                           per shard  ‒  2 MB/s per shard per consumer (enhanced fanout)

             kinesis data firehose: batch writes (near real time) - 32MB or 60s
                                    custom data transform with lambda
                                    => S3
                                    => redshift (via S3)
                                    => ElasticSearch (now OpenSearch)

SQS
visibility timeout (30s): message invisible to other consumers,
ChangeMessageVisibility API call if not done processing

MaximumReceives: times a msg is allowed to go back to the queue,
                 then move it to DLQ (dead letter queue)

Delay queue: hide message for up to 15min, send with DelaySeconds can override this

long polling (up to 20s) => less API calls. enable at Q level or WaitTimeSeconds API

request-response systems:
the producers (requesters) send messages ( [ID/response Q (answer expected there)] ) to a single request Q,
the consumers (responders) reply via many virtual Qs (SQS Temporary Queue Java Client needed)

ordering:
FIFO + group ID: block group A messages for other consumers while a group A
                 batch is in flight (being processed, eg. A3-A2-A1), else a
                 consumer could process say A4 before and the ordering would be broken => A3-A2-A1-A4

SNS

           topic,
publish to phone (SMS),
           platform endpoint (e.g ADM: Amazon device messaging)

100_000 topics -> 12_500_000 subscriptions per topic (optional JSON policy to filter messages)

FIFO topic: ordering of messages per group
            subscribers can only be SQS FIFO (throughput limited to 300/s)

Elastic Container Service

ECS cluster
   container instances (e.g EC2)
      services - our app will be a versioned service (v1, v2, ...)
          tasks - tasks are isolated in services, many services can be defined on the same container instances.

launch types
*     EC2: 1 ECS agent        per instance, the agent will use the EC2 instance profile role <-> ECS, ECR, CloudWatch
* Fargate: 1 ENI (private IP) per task,     the task  will use  an ECS task role

share data among tasks by mounting EFS volumes onto the tasks

ALB - TG:
individual processes run on separate EC2 instances =>
if ALB listens on port 80, the process can also listen on port 80

ALB - service on EC2:
multiple tasks can reside on the same container instance => if ALB listens on
port 80, all tasks can't listen on port 80 so they listen on random ports which
the ALB will automatically find, therefore on the container instance's SG we
must allow all ports from ALB's SG

ALB - service on Fargate:
on the ENIs SGs allow the task port from the ALB SG

scaling

CloudWatch alarm (e.g on CPU service usage) -> service auto scale -> [for EC2 launch type we would also need an ASG for the container instances]

CloudWatch

Metrics
* namespaces (e.g EC2) + up to 10 dimensions (identification attributes)
* custom metrics: PutMetricData API call (accepts data points 2 weeks in the past & 2h in the future)
                  StorageResolution - 1min (std), 1/5/10/30s (high)
* metric filter: metric based on CloudWatch Logs filter

EC2: metrics every 5 min or every 1 min (std StorageResolution) with detailed monitoring, no RAM metric

Logs

- query logs with CloudWatch Logs insights
- unified agent (EC2/on-prem, old: logs agent)
    - extra system-level metrics
    - centralized configuration with SSM parameter store
- export (up to 12h to become ready): CreateExportTask API call, not real or near-real time
- subscription filter: real time (pub/sub)

while exploring metrics in the console, I couldn't filter for sqs, it only worked by filtering for aws first! bug (???)

Events

EventBridge (old CloudWatch events):
* can intercept any AWS events and define action targets for them.
* define CRON jobs (execute task with lambda)

event bus

1. default: for AWS services
2. partner: receive events from 3rd party (send events too???)
3. custom:  own bus

schema registry: collection of JSON events to help generate code

CloudTrail

90 days retention for events
* management events (e.g CreateSubnet; can separate Read/Write)
* data events (e.g GetObject; not logged by default)

enable insights to continuously analyse management write events in order to detect unusual activity => console
                                                                                                    => S3
                                                                                                    => EventBridge event

AWS Config
* record configuration changes
* evaluate compliance rules (managed or custom with lambda: e.g are all EBS disks of type io2?)
                      +- eval/trigger per change or at intervals
                      +- remediation of non-compliant resources with SSM automation documents (managed or custom)

NOT serverless

RDS, Aurora (can be), Redshift, ElastiCache: you have to provision the EC2 instance/node type

Lambda

limits per region: 128 MB - 10 GB
                   15 min (900s)
                   1000 concurrent executions
                   env    4 KB
                   /tmp 512 MB
                   size  50 MB compressed or 250 MB uncompressed

Step Functions

orchestrate your Lambda functions: visual workflow or JSON state machine
* maximum execution time of 1 year.
* possibility to implement human approval feature

use AWS SWF (Simple Workflow Service, EC2 => not serverless) instead if:
- you need external signals
- you need child processes

DynamoDB
react to changes by enabling streams (and we get 24h data retention)

API Gateway vs ALB

- edge-optimized (CloudFront) by default
- serverless + we can add an ALB (not for lambda as it scales/balances automatically)
- environments (dev/test/prod)
- authentication & authorization
- request throttling/transform
- caching
- expose any AWS service

security
- internal: IAM permissions in headers (leverages sig v4)
- 3rd party (OAuth, SAML): token in headers, validate with lambda authorizer and return IAM policy (can be cached)
- CUP: authentication only

Cognito

federated means 3rd party source (e.g Google, Facebook)

Authenticate to app
* Cognito User Pools (CUP is an IdP, an identity provider: serverless db of users)
  sign-in (verif, MFA, ...) -> JSON web token

Authorize to use AWS resources
* Cognito Identity (role) Pools (credentials provider, prefer to AssumeRoleWithWebIdentity)
  login to get token from IdP (Facebook, CUP, ...)
  Identity Pool verifies token and gets IAM creds from STS

AppSync (old Cognito sync): save app state (20 datasets - 1MB), devices sync, offline, id pool needed

AWS STS

grant limited and temporary access to resources (token valid for 15min - 1h)

* AssumeRole... STS APIs
   - AWS: dev account -- assume UpdateProdBucket role: STS gives token --> modify prod account bucket
   - 3rd: IdP (e.g ADFS) sends SAML assertion, AssumeRoleWithSAML,            STS returns temp creds
                                               POST assertion to SSO endpoint
* GetSessionToken (for MFA)

federation with SAML 2.0 is the old way, prefer SSO federation

SSO

when we need to login to:
* many AWS accounts
* many 3rd party business apps (Slack, Dropbox, Office 365, ...)
* many custom SAML applications
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
              +- with SSO no need to manage all these portals, we connect directly to the Id store

Directory Services

Microsoft AD: centralized users/assets management from the domain controller

* managed Ms AD: on-prem <=> AWS - manage users on both (MFA supported)
* AD connector:  on-prem <=      - proxy to on-prem AD, manage all users in there
* simple AD:       N/A       AWS - AD-compatible, manage on AWS, no on-prem connection

Organizations

Root OU
   master account
      OU + member accounts

create accounts + OU (organizational units) per BU(business unit)/env/project/...

OU aren't accounts, they just help structuring the hierarchy

SCP (service control policies, restrictive by default)
* whitelist/blacklist IAM actions at the OU/account level
* does not affect service-linked roles

move account to another organization: delete from current, invite from 2nd

IAM

when you assume a role, you give your original permissions!

permission boundary (user, role, NOT group)
ex: if boundary = allow s3:*           on *
                  allow iam:CreateUser on * won't work
useful to restrict one specific user instead of a whole account with SCP

Service-Linked Role:
only a specific service can use this role vs a regular role which can be used by all services/users

principal - user, app, service
condition - aws:SourceIP, aws:RequestedRegion, ec2:ResourceTag
            "Bool" or "BoolIfExists" (MFA doesn't apply to all resources): {"aws:MultiFactorAuthPresent": false}

* arn:aws:s3:::my-bucket   => bucket level permission (e.g ListBucket)
* arn:awn:s3:::my-bucket/* => object level permission (e.g Get/PutObject)

Resource Access Manager

avoid resource duplication: share resources with any account

VPC subnets
* share within organization only
* network is shared => access via private IPs (cross-account SGs can be referenced but not viewed)

Encryption

KMS

share passwords/credentials/certificates/encryption at rest
data > 4KB => use envelope encryption
access: MANDATORY Key policy + optional IAM policy
                  +- default (complete access to root + allows access with IAM policies)

* AWS services use symmetric AES-256 CMK (customer master key) keys [create
  - AWS managed (free)                                               enable/disable
  - customer managed                                                 rotate]
    +- automatic rotation: once a year
    |  - same key id, new backing key (keep old one)
    +- manual rotation: if greater frequency needed or CMK is asymmetric so not eligible for automatic rotation
       - new key id, new backing key (keep old one)
       - apps use the key id so we need an alias to the id in this case
  - customer imported

* RSA, ECC (elliptic-curve cryptography) asymmetric keys are used for:
  - sign/verify integrity checks
  - outside of AWS (no access to KMS API)

SSM Parameter Store

secure storage for configuration and secrets, version tracking
ssm.get_parameters(Names=['/site/prod/db-url'], WithDecryption=True)

std vs advanced (TTL in parameters policies, more params of bigger size + higher throughput)

AWS Secrets Manager (mostly for RDS MySQL, PostgreSQL, Aurora)

* rotation of secrets + new auto generation
* KMS encrypted

Shield

Route 53, CloudFront, Global Accelerator, ELB, EC2

WAF (web app firewall)

ALB, API Gateway, CloudFront)

web ACLs:
* IP filtering
* http based rules (header, body, URI str)
* rate (DDoS) + geo-match rules
* SQL injection + XSS (cross-site scripting)

AWS Firewall Manager

common set of security rules at the organization level:
- WAF             (ALB,     CloudFront, API Gateway)
- Shield advanced (ALB/CLB, CloudFront, Elastic IP)
- SG              (EC2 + ENI)

GuardDuty

threat discovery (cryptocurrency attacks): uses ML, anomaly detection, 3rd party data
input: DNS/VPC Flow/CloudTrail logs

Amazon inspector

EC2 - agent     => OS vulnerabilities, CIS (center for internet security) benchmarks
      agentless => network accessibility
inspector service to send report via SNS

Amazon Macie

ML + pattern matching to alert about exposed (e.g in S3) sensitive data (PII: personally identifiable information)

VPC

IGW + routing table for the public subnets = Internet connectivity

* 5 per region (soft limit)
* 5 CIDR per VPC: min /28 (    16 IPs)
                  max /16 (65 536 IPs)
* reserved addresses (e.g 10.0.0.0/24)
  - 10.0.0.0: network
  - 10.0.0.1: router
  - 10.0.0.2: DNS (or 169.254.169.253)
  - 10.0.0.3: future use
  - 10.0.0.255: broadcast (not supported in VPC!)

best practice:
           VPC /16 - 65 536
 public subnet /24 - 256 (we don't need too many hosts in a public subnet)
private subnet /20 - 4096

NACL (Network Access Control List aka subnet firewall)

* stateless: always needs in + out rules
* rules are ordered from lowest to highest number, 1st one wins (low num = high precedence)
* good way of blocking a specific IP address at the subnet level
* default NACL => allow everything, new NACL => deny everything
* best practice: use increments of 100 to allow room for more rules

ephemeral ports

- clients connect to a defined port, and expect a response on an ephemeral port
- because NACL are stateless, we lose info about source port of incoming
  traffic, therefore outbound return traffic must go to all ephemeral ports:

  allow TCP/3306      to   db subnet --> --> allow TCP/3306      from web subnet
  allow TCP/ephemeral from db subnet <-- <-- allow TCP/ephemeral to   web subnet

DNS support/resolution (true by default):
it's best to have a DNS server within the VPC to avoid unnecessary network traffic
route 53 (default) or custom

DNS hostnames (false by default, true for default VPC):
* needs enableDnsSupport=true
* if true => add public DNS for public instances

both needed for custom private DNS

In public subnet:

- bastion host => connect to private EC2 instances

  make it highly available:
  * multi-AZ NLB (as ssh is layer 4) - ASG 1:1:1
  * bonus - thanks to the NLB, the bastion can be moved to the private subnet

- NAT (EC2) instance (deprecated) => Internet connectivity for private EC2 instances
  * disable source/destination check (can forward traffic)
  * must have elastic IP
  * private subnets to route via it

- NAT Gateway
  * elastic IP
  * single AZ (must create multiple NAT Gateways in multiple AZs for HA)
  * no SG to manage
  * 5 to 45 Gbps auto-scaling bandwidth
  * can't be used as bastion host

A private host behind NAT "can't" be contacted, for that you need NAT traversal:
Also known as UDP encapsulation, it allows traffic to get to the specified
destination which doesn't have a public IP address. In a S2S VPN connection, a
CGW behind NAT needs NAT-T enabled

VPC Peering: VPCs must not have overlapping CIDRs

VPC Endpoints (AWS PrivateLink)

* interface endpoints: ENI (private IP => SG)
* gateway endpoints (at no cost!): S3, DynamoDB

- connect to AWS services privately (from within your private subnets)
- DNS support must be on, route tables will need amending,
  no need for IGW or NATGW

VPC Endpoint Services (PrivateLink)

expose your own services (not AWS ones as above) through a NLB (or GWLB),
then consumers can connect via ENI thanks to PrivateLink

Flow Logs

troubleshoot SG & NACL issues

 inbound accept (SG outbound accept), outbound reject => NACL
outbound accept (SG  inbound accept),  inbound reject => NACL

                       /     IPs     \     /    ports    \                               / epoch \
ver | account | eni | srcaddr | dstaddr | srcport | dstport | proto | packets | bytes | start | end | action | status
                                                                                                      v   v
                                                                                                      SG, NACL - ACCEPT/REJECT

capture IP traffic
* VPC, subnets, ENI
* ELB, RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway... (managed interfaces)

Traffic Mirroring

capture actual IP traffic for deeper inspection: tcpdump
e.g from EC2 ENI to another ENI or NLB

EC2-Classic (deprecated)

pre-VPC era: instances ran in a single network shared with other customers.
to link those old instances to our VPC, we need ClassicLink

Site-to-Site (S2S)

VPN:
IPsec over the public Internet

Direct Connect (DX):

* unencrypted private connection (add VPN between DX location and DC to have IPsec encryption)

* 1 month to setup connection
  - dedicated (1Gbps and 10Gbps)
  - hosted (capacity on-demand: 50Mbps, 500Mbps, 1, 2, 5 to 10Gbps)

* high resiliency:    multiple DX locations - multiple DCs
  maximum resiliency: multiple DX locations - multiple DCs
                       separate connections - separate connections
                       per location           per DC

    virtual private gateway - VPG gw or DX gw
   /
VGW                  <--> CGW (customer gw)
                     <--> CGW (CloudHub hub-and-spoke model if multiple DCs)
     [ DX location ] <--> customer router in DC
       AWS|customer

VGW region 1 \
              <--> DX Gateway <-> DX <-> DC (direct connect for same region, direct connect gw cross-regions)
VGW region 2 /

AWS side
* must enable route propagation so subnets know how to contact the VPN gateway
* VPN concentrator (device that helps to manage multiple VPN connections => VPN on a larger scale)
* allows for custom ASN (Autonomous System Number). Edge location???

on-premises
* enable NAT-T if behind NAT

Transit Gateway (IP multicast)

transitive peering (traffic passes through) of multiple VPCs,
can also transfer VPN and DX Gateway connections.

* share cross-account with RAM
* peer with other Transit Gateways across regions
* use route tables to limit communications

ECMP (equal-cost multi-path)

define multiple S2S VPN connections to increase the bandwidth of your connection to AWS

1x VPN gw = 2 tunnels = 1.25Gbps
2x                         5Gbps
3x                       7.5Gbps

IPv6

* IPv6 addresses are public and Internet-routable (no private range)
* egress-only IGW => same effect as a NAT gw but IPv6 are public so no NAT is needed
* 2001:db8::1234:5678 -> the middle 4 segments are zero
* IPv4 + IPv6 = dual-stack mode

Networking Costs in AWS

free for ingress traffic, we pay only when exiting AWS network

*  free  with private IPs within AZ
* $0.01  with private IPs  cross AZ
* $0.02  with  public IPs  cross AZ
* $0.02                    inter region
* $0.09  (S3)                    Internet
* $0.085 (S3 + CloudFront)       Internet (actually cheaper and S3 requests are 7x cheaper => way better than previous row)

Disaster recovery

examples below: on-premises to AWS cloud

- RPO: Recovery Point Objective => minimize data loss
  disaster
- RTO: Recovery  Time Objective => minimize downtime

• Backup and restore                                  - backup/restore from snapshots:  cheapest ->   high RPO + RTO
• Pilot light  (bare core up in the cloud)            -                    DB replica:     cheap ->  lower RPO + RTO
• Warm standby (full min-size system up in the cloud) -        ELB + ASG + DB replica: expensive ->    low RPO + RTO
• Hot site     (full     size system up in the cloud) -        ELB + ASG + DB replica:    COSTLY -> lowest RPO + RTO
  multi site active-active approach

chaos: test your prod setup (ref. Netflix simian-army)

Migration to AWS

* VM import/export (VMs <-> EC2, or ami.iso to use on-premises)
* Migration Hub
* Application Discovery Service
*      Server Migration Service (SMS, live DMS-like migration)
*    Database Migration Service (section below)

DMS (database migration service)

source -- EC2 with DMS -- destination
+- all dbs                +- all dbs
+- S3                     +- S3
                          +- ElasticSearch
                          +- Kinesis data streams

* for heterogeneous migrations (different db engines), SCT (Schema ConversionTool) is needed beforehand.
* continuous data replication with CDC (change data capture): source remains available

DataSync

move large amounts of data to AWS (storage gw < DataSync < DX/snow family)

* on-prem NAS to AWS S3/EFS/FSx
                                                => S3
NAS (NFS/SMB) + DataSync agent --> AWS DataSync => EFS
                                                => FSx for Windows file server
* AWS: EFS to EFS

 EFS + EC2 with DataSync agent --> AWS DataSync => EFS

AWS Backup

centralize AWS snapshots management:
* we need a plan (frequency + retention policy) and AWS services => it all goes to S3
* supports PITR (point in time recovery), tag-based backups, ...

HPC (high performance computing)

EC2 Enhanced Networking (SR-IOV): single root i/o virtualization:
single NIC to present itself as several virtual NICs
* ENA: higher PPS (packets per second) - or legacy Intel 82599 VF for up to 10Gbps
* EFA: enhanced ENA leveraging MPI (message passing interface)
                               +- bypasses the underlying Linux OS for lower latency

AWS Batch: multi-node (EC2) parallel jobs
AWS ParallelCluster: open source cluster management tool for HPC

CICD CodePipeline

find/fix bugs early, deploy often
*                   push to CodeCommit - GitHub
*           build & test in CodeBuild  - Jenkins CI (continuous integration)
* deploy passing build with CodeDeploy - Jenkins CD (continuous delivery: create packages)
* provision with CloudFormation and/or Ansible (actual deploy???)

CloudFormation

IaC (infrastructure as code)
* YAML templates go in S3, deploy stack via cli
* figures out the right order of creation (declarative programming)
* estimate costs thanks to resource tags using the CloudFormation template
* dev env: save money by auto deleting 5pm / creating 8am templates
* auto diagrams

template
- AWS resources
- parameters: dynamic inputs
- mappings:   static vars
- outputs
- conditionals
- metadata

StackSets

Manage stacks across multiple accounts/regions with a single operation.
Update a stackset to update all stack instances.

EMR (elastic MapReduce): manage hadoop clusters to process/analyze big data
Opsworks: managed Chef & Puppet (alternative to AWS SSM)
WorkSpaces: VDI (Virtual Desktop Infrastructure), managed, secure cloud desktop (Linux/Windows)
AppSync: store and sync data across mobile and web apps in real-time (uses GraphQL from Facebook)
Cost Explorer: Savings Plan alternative to Reserved Instances

Well Architected Framework 5 Pillars

* Operational excellence: IaC, anticipate failure
  CloudFormation, AWS Config, monitoring, CICD

* Security
  IAM, security at all levels, encryption, keep people away from data

* Reliability
  stop guessing capacity => ASG, test/automate recovery

* Performance efficiency
  use serverless, stay up-to-date: AWS News Blog

* Cost optimization
  Cost Explorer, Trusted Advisor, spot instances
                 |
                 • cost optimization
                 • performance
                 • security
                 • faulttolerance
                 • service limits/quotas

https://aws.amazon.com/architecture/
https://aws.amazon.com/solutions/

Exams

* RDS read replicas free between AZs, not regions
* ASG scale-in, increase deregistration delay to not interrupt long running processes
      launch template for multiple instance types (on-demand + spot)
* producers for firehose: SDK, agent + data streams/CloudWatch/Iot => kinesis data firehose
* blue-green deployment: DNS routing or Global accelerator
* EFS: file storage, S3: object storage
* By default, an S3 object is owned by the AWS account that uploaded it => the S3 owner might not have permission to view the objects
* spot fleet, no auto scaling capabilities?
  ASG with launch template for mix of on-demand/spot
