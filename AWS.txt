EC2

tenancy:
* VPC dedicated tenancy => EC2 dedicated
* VPC   default tenancy => EC2 dedicated only if the launch config says so, else shared

EC2 tenancy can change from dedicated to host and vice-versa

Reserved Instances vs Capacity Reservations

Use reserved instances if you want to commit to 1 or 3 years

Amazon EC2 on-demand Capacity Reservations enable you to create and manage
reserved capacity on Amazon EC2 without any long-term commitment or fixed
terms. This can be very beneficial if you regularly face
InsufficientInstanceCapacity errors when AWS doesn't have enough available
on-demand capacity while starting or launching an EC2 instance.

For cluster placement groups, capacity also means running on the 'same'
hardware and that hardware needs to be reserved.

Placement Groups

  cluster -  low latency,  low availability: share hardware => same EC2 type                                                                     <= HPC
partition -  avg latency,  avg availability: spread partitions across hardware, 7 partitions per AZ                                              <= Big Data
   spread - high latency, high availability: multi AZ, 7 instances in different racks in 1 group per AZ, diff hw allows for different EC2 types  <= HA

curl http://169.254.169.254/latest/meta-data/

ELB (Elastic Load Balancer)

LBs are managed and highly scalable => behind the scenes there are many LB instances.
cross-zone loadbalancing: traffic distributed evenly across all targets in all AZ (NLB: disabled by default + $)

ALB
* distribute load based on http/https/websocket (stateful unlike http)
* connection/SSL termination

dispatch requests to target groups based on routes, hostnames, query string, headers (ALB)
                                         /url1 -> tg1
                        /url2, one.example.com -> tg2
two.example.com, ?platform=mobile, HTTP header -> tg3

then spread load to targets on multiple/single (e.g containers) machine(s) in the
target group, based on health checks (port + route) =>
seamless handling of downstream instances failures

NLB
* distribute load based on TCP, UDP, TLS + port
* pass through (but TLS offloading possible), request IP goes all the way to the app
* static IPs per AZ => customers can whitelist us

Networking security

  SG - statefull, if one way is allowed then the return way is automatically allowed
NACL - stateless, both ways are always evaluated

services behing SGs:
EC2, ELB, EFS, RDS, ElastiCache

CloudFront (CDN - cache content at the edge)

defenses:
- endpoint ELBs will only see CloudFront IPs, not client ones =>
  NACL has no effect on allowing/denying traffic. our only line of defense is WAF
- whitelist/blacklist geo restriction

origins
* S3
* any http/https/rtmp endpoint
                 +- real-time messaging protocol

multi-origins based on /path/*
origin groups (primary/secondary) for failover

price classes:
class all - all regions
class 200 - all, without the most expensive ones
class 100 - least expensive regions only

field level encryption:
extra security on top of https - specify up to 10 fields in your POST request
asymmetric encryption (e.g credit card details) on the edge, decrypted by app's (e.g behind ALB origin) private key

Global Accelerator (use edge locations to accelerate APPS (no S3), no caching)

It's a global load balancer!
region endpoint groups (akin to TG) => health checks => fast regional failover
                +- e.g ELBs but can also be EC2s

* UDP, IoT (MQTT), VOIP endpoint
* http/https            endpoint (if static IPs needed: 2 anycast IPs)

blue-green deployment: both DNS routing and global accelerator can be used

EBS
                            max
gp3 | 1 GiB to 16 TiB |  16 000  iops | 1000 MiB/s | not multi-attach
io2 | 4 GiB to 16 TiB |  32 000 piops |            |
    |                 |  64 000 piops |            | + Nitro
    | 4 GiB to 64 TiB | 256 000 piops |            | + block express

encrypt unencrypted EBS volume / RDS database - create a snapshot, then:
* create new encrypted EBS volume from it
* copy it into an encrypted one, create new volume/db from it

shared storage (we need to create mount ENI targets, to be mounted on EC2, ...):
* EFS (NFS),
* FSx for Lustre (Linux cluster)
* FSx for Windows (SMB, NTFS)

FSx persistent file system: data is replicated
FSx scratch    file system: temp storage, faster, cheaper

Storage Gateway (on-premises -> S3)

hybrid storage integration (storage gateways needed because S3 proprietary):
on-premises                                                      | cloud
-----------------------------------------------------------------+--------------------------------------
 app server,     file gateway (NFS, SMB), IAM, optional AD auth  | S3, S3 IA (both) -> S3 IA, S3 glacier
             FSx file gateway (SMB, NTFS, AD), cache             | FSx for Windows file server
 app server,   volume gateway (iSCSI)                            | S3               -> S3 EBS snapshots
data server,     tape gateway (iSCSI, VTL (virtual tape library) | S3               -> S3 glacier

volume gateway specific
cached volumes: main data is on S3 with local on-prem cache for fast access
stored volumes: main data is on-prem with async backup to S3

virtualization is needed (???) to install the gateways, instead we can buy a HW appliance

ASG

- use a launch template to provision a mix of on-demand & spot instances
- increase deregistration delay to not interrupt long running processes when scaling-in

EC2 instances are in one of the following states:
- InService
- Standby: helps you temporarily remove an instance from the ASG

EC2 status check (ok or impaired)
* instance status check
*   system status check => AWS responsibility to repair

a recovered instance is almost identical to the original one
- preserved: ID + metadata, IPs (private, public, Elastic)
-      lost: RAM data

after scale in/out activites ASG enters the HealthCheckGracePeriod,
allowing health checks to stabilize before launching/terminating more instances

Use golden AMI so updates, app install, ... take less time => we can set a smaller HealthCheckGracePeriod (aka cooldown period)

Spot instances

* a persistent spot request is like an ASG. it will keep launching/terminating instances till the end of its validity period
* you can only cancel spot requests that are open, active, or disabled!
* if a spot request is persistent, then it is reopened after your spot instance is interrupted (not stopped)
* spot blocks (instances) with a defined duration (1, 2, 3, 4, 5, or 6h) are designed not to be interrupted

RDS

In AWS there is a network cost when data moves between AZs,
but not for read replicas (only cross-region)

Aurora DB

auto scaling storage (10GB - 128TB, redshift: 1-128 nodes each up to 128TB)
writer + reader OR custom endpoint(s)

read replicas:
* each replica is associated with a priority tier (0-15)
* failover: promote replica with lowest-tier & max-size (highest priority combination)

Redshift

* no multi-AZ: better enable automated cluster snapshots cross-region COPY (every 8h, 5GB or on a schedule) for DR
* spectrum: perform queries directly against S3 (no need to load)
* enhanced VPC routing: stay within VPC, no public Internet

MPP (Massively Parallel Processing)
both redshift and athena use Presto (distributed SQL query engine)

ElastiCache

* heavy code changes required
* no IAM auth, Redis auth or Memcached SASL

Redis only features:
* advanced data structures (e.g sorted sets for real-time leaderboards)
* snapshots
* replication
* transactions
* pub/sub
* lua scripting
* geospatial support

Memcached only feature:
multithreaded architecture

Route 53

Alias: CNAME to 1 managed AWS resource (no EC2!)
       no TTL, can point to zone apex, free

record with multiple A values -> the client will choose at random (client side LB)

health checks: only return IPs for healthy resources
               e.g give me a healthy ALB, then target group health check to give me a healthy EC2 instance

routing policies:

- simple (no health checks)
- weighted
  weight.example.com 70 - 7.8.9.1
  weight.example.com 30 - 3.4.5.6
  weight.example.com 10 - 1.1.8.8
- latency
- failover (primary active / secondary passive)
- geolocation (default IP mandatory)
- geoproximity (traffic flow, bias -1 .. 99)
- multi-value (again client side LB but with health checks, return up to 8 IPs)

GoDaddy registrar with Route 53 DNS:
register domain with GoDaddy but specify custom nameservers (AWS ones) where the records will be defined

Beanstalk

dev centric view, infrastructure is transparent
PaaS: versioned application / environment (dev,test,prod) +    web tier (ELB -> ASG) or
                                                            worker tier (SQS <- ASG)

you retain full control over the provisioned AWS resources and can access them at any time

S3 (object storage, web URL - http/https)

- object storage (vs file system) does not allow for in-place edits => not good for collaboration.
- by default, an S3 object is owned by the AWS account that uploaded it => the S3 owner might not have permission to view the objects

3_500 PUT req/s per prefix
5_500 GET req/s per prefix, both limited by KMS (5_500, 10_000, 30_000 req/s based on region, increase with quotas)

naming: 3-63 -> no upper, _, IP; start with [a-z0-9]
        s3://bucket-name/folder-1/folder-2/my-image.jgp - max 5TB, multi-part upload if >5GB
                         prefix          + name = key

static website endpoint (.region or -region):
http://my-bucket.s3-website.region.amazonaws.com
http://my-bucket.s3-website-region.amazonaws.com

with versioning enabled, removal of an object adds a 'delete marker'.
deleting a specific version or a 'delete marker' one is permanent.

you can place a retention period on an object version either explicitly (Retain Until Date) or through a bucket default setting.
like all other object lock settings, retention periods apply to individual object versions

storage classes:
std | intelligent-tiering | std-ia | 1 zone-ia | glacier | glacier deep archive

        std, std-ia -> 30 days min stay before transition to std-ia or 1 zone-ia
intelligent tiering -> small monthly monitoring and auto-tiering fee
     amazon glacier -> vaults/archives naming
                       90 days min charge, 180 for deep archive, others 30 (bar std)
retrieval cost per GB for all but std/intelligent

lifecycle rules
- transition actions
- expiration actions (deletion)

replication isn't chained:
A -> B -> C doesn't mean A -> C.
objects in B replicated from A aren't considered new. only explicit new ones will be replicated to C

encryption:
metadata is NOT encrypted

SSE-S3  = "x-amz-server-side-encryption": "AES256",  in header
SSE-KMS = "x-amz-server-side-encryption": "aws:kms", in header
SSE-C   =                                       key, in header (https mandatory)
        => CloudHSM (hardware security module, must use client software)
           * single-tenant, multi-AZ
           * FIPS 140-2 Level 3 (Federal Information Processing Standard)
           * MFA + access & authentication management (users & keys) vs IAM
           * hardware acceleration
           * supported by Redshift
CSE     = client side encryption (could use the Amazon S3 Encryption Client)

the default encryption setting will be applied only to non-encrypted objects,
meaning that if an object is already encrypted (e.g via bucket policy) it won't be altered.

block all public access
* to buckets/objects                 via new ACLs
* to buckets/objects                 via ANY ACLs (existing ones too)
* to buckets/objects                 via new public bucket or access point policies
* to buckets/objects + cross-account via ANY public bucket or access point policies

pre-signed URLs
generate GET ones with cli, GET/PUT ones with SDK (creator's get/put permissions inherited by users)
valid for a limited time only (3600s by default)

with CloudFront in front of our bucket, we must use CloudFront signed (e.g use lambda@edge):
* URLs for single files
* signed cookies for multiple files
because bucket access is restricted to the OAI

CORS: cross-origin resource sharing (web browser security mechanism)
get index.html                         from www.example.com (origin - protocol://domain:port),
    index.html tries to get a resource from   net.games.com (cross-origin)
                                              net.games.com needs to send headers Access-Control-Allow-Origin:  https://www.example.com
                                                                                  Access-Control-Allow-Methods: GET, ...

S3TA (S3 Transfer Acceleration) is preferred over CloudFront + S3 for content bigger than 1GB

EFS (file storage)

protect EFS with:
* access points: manage app access
  override clients uid/gid then use rwx permissions (clients uid/gid trusted by default)
* VPC SGs to control traffic to and from the file system
* IAM policy for mount permissions (who can mount the fs)

Snow family (EBS and S3 storage)

             snowcone (  8TB)             2  CPU,   4GiB RAM, no battery/cables, can use DataSync once online
snowball edge compute ( 42TB) optimized: 52 vCPU, 208GiB RAM, optional GPU
snowball edge storage ( 80TB) optimized: 40 vCPU,  80GiB RAM, up to 15 nodes storage cluster, cannot import to glacier directly
           snowmobile (100PB) - prefer to snowball if >10PB

OpsHub: AWS 'snow' Console on your laptop

Async app to app communication

                         queue model: SQS (256kb per msg, 4 to 14 days retention) <- poll for up to 10 messages
                       pub/sub model: SNS
real-time streaming (~pub/sub) model: kinesis data streams (records with partition key, same key goes to same shard => ordering can be achieved)

Kinesis Data streams (retention: 1(default) - 365days, replay...)

        producers                     consumers
kinesis agent, SDK, KPL  ‒  SDK, KCL (=> EC2, lambda, ...)
 1 MB/s (or 1000 msg/s)  ‒  2 MB/s per shard per all      (shared)
              per shard  ‒  2 MB/s per shard per consumer (enhanced fanout)

Kinesis Data Firehose

producers: SDK, agent, data streams/logs/events/IoT
consumers: batch writes (near real time) - 32MB or 60s
  => S3
  => redshift (via S3)
  => ElasticSearch (now OpenSearch)

custom data transform with lambda

SQS
visibility timeout (30s): message invisible to other consumers,
ChangeMessageVisibility API call if not done processing

MaximumReceives: times a msg is allowed to go back to the queue,
                 then move it to DLQ (dead letter queue)

- delay queue: postpone all new messages for up to 15min,
               send with DelaySeconds can override this
- message timer: delay period for a single message

long polling (up to 20s) => less API calls. enable at Q level or WaitTimeSeconds API

request-response systems (producers want a response back):
the producers (requesters) send messages ( [ID/response Q (answer expected there)] ) to a single request Q,
the consumers (responders) reply via many virtual Qs (SQS Temporary Queue Java Client needed)

ordering:
FIFO + group ID: block group A messages for other consumers while a group A
                 batch is in flight (being processed, eg. A3-A2-A1), else a
                 consumer could process say A4 before and the ordering would be broken => A3-A2-A1-A4

SNS

         +- topic,
publish -+- phone (SMS),
         +- platform endpoint (e.g ADM: Amazon device messaging)

100_000 topics -> 12_500_000 subscriptions per topic (optional JSON policy to filter messages)

FIFO topic: ordering of messages per group
            subscribers can only be SQS FIFO (throughput limited to 300/s -> up to 3000/s in batch mode with batches of 10)

ECS (Elastic Container Service)

launch types
* Fargate: 1 ENI (private IP) per task,     the task  will use  an ECS task role
*     EC2: 1 ECS agent        per instance, the agent will use the EC2 instance profile role <-> ECS, ECR, CloudWatch

ECS cluster
   container instances (e.g EC2)
      services - our app will be a versioned service (v1, v2, ...)
          tasks - tasks are isolated in services, many services can be defined on the same container instances.

share data among tasks by mounting EFS volumes onto the tasks

ALB - TG:
individual processes run on separate EC2 instances =>
if ALB listens on port 80, the process can also listen on port 80

ALB - service on EC2:
multiple tasks can reside on the same container instance => if ALB listens on
port 80, all tasks can't listen on port 80 so they listen on random ports which
the ALB will automatically find, therefore on the container instance's SG we
must allow all ports from ALB's SG

ALB - service on Fargate:
on the ENIs SGs allow the task port from the ALB SG

scaling

CloudWatch alarm (e.g on CPU service usage) -> service auto scale -> [for EC2 launch type we would also need an ASG for the container instances]

CloudWatch

Metrics
* namespaces (e.g EC2) + up to 10 dimensions (identification attributes)
* custom metrics: PutMetricData API call (accepts data points 2 weeks in the past & 2h in the future)
                  StorageResolution - std: 1min, high: write 1s
                                                        read 1/5/10/30s
* metric filter: metric based on CloudWatch Logs filter

EC2: metrics every 5 min or every 1 min with detailed monitoring, no RAM metric

Logs

- query logs with insights
- unified agent (EC2/on-prem, old: logs agent)
    - extra system-level metrics
    - centralized configuration with SSM parameter store
- export (up to 12h to become ready): CreateExportTask API call, not real or near-real time
- subscription filter: real time (pub/sub - lambda, kinesis)

EventBridge (old CloudWatch events)

it can be used to simulate SQS between 3rd parties (SaaS)

* can intercept any AWS events and define action targets for them.
* define CRON jobs (execute task with lambda)

event bus

1. default: for AWS services
2. partner: receive events from 3rd party (send events too???)
3. custom:  own bus

schema registry: collection of JSON events to help generate code

CloudTrail

90 days retention for events
* management events (e.g CreateSubnet; can separate Read/Write)
* data events (e.g GetObject; not logged by default)

enable insights to continuously analyse management write events in order to detect unusual activity => console
                                                                                                    => S3
                                                                                                    => EventBridge event

AWS Config

* record configuration changes
* evaluate compliance rules (managed or custom with lambda: e.g are all EBS disks of type io2?)
                      +- eval/trigger per change or at intervals
                      +- remediation of non-compliant resources with SSM automation documents (managed or custom)

Lambda

400_000 GB-seconds of compute time per month for free:
400_000 seconds if function is 1GB RAM

limits per region: 128MB - 10GB RAM
                   15min (900s)
                   1000 concurrent executions
                   env    4KB
                   /tmp 512MB
                   size  50MB compressed or 250MB uncompressed

Step Functions

coordinate and orchestrate multiple AWS services (lambda, glue, ...) into serverless workflows (visual or JSON state machine)
* maximum execution time of 1 year.
* possibility to implement human approval feature

use SWF (Simple Workflow Service, EC2 => not serverless) instead if:
- you need external signals
- you need child processes

DynamoDB

react to changes by enabling streams (and we get 24h data retention)

API Gateway (vs ALB)

- edge-optimized (CloudFront) by default
- serverless + we can add an ALB (not needed for lambda since lambdas spring into existence => there is always an 'idle' lambda ready to take on load => LB is N/A)
- environments (dev/test/prod)
- authentication & authorization via cognito
- request throttling/transform
- caching
- expose any AWS service

security
- internal: IAM permissions in headers (leverages sig v4)
- 3rd party (OAuth, SAML): token in headers, validate with lambda authorizer and return IAM policy (can be cached)
- CUP: authentication only

Cognito (works with API gw, ALB, not CloudFront)

federated means 3rd party source (e.g Google, Facebook)

Authenticate to app
* Cognito User Pools (CUP is an IdP, an identity provider: serverless db of users)
  sign-in (verif, MFA, ...) -> JSON web token

Authorize to use AWS resources
* Cognito Identity (role) Pools (credentials provider, prefer to AssumeRoleWithWebIdentity)
  login to get token from IdP (Facebook, CUP, ...)
  Identity Pool verifies token and gets IAM creds from STS

AppSync (old Cognito sync): save app state (20 datasets - 1MB), devices sync, offline, id pool needed

AWS STS

grant limited and temporary access to resources (token valid for 15min - 1h)

* AssumeRole... (STS APIs)
   - AWS: dev account -- assume UpdateProdBucket role: STS gives token --> modify prod account bucket
   - 3rd: IdP (e.g ADFS) sends SAML assertion, AssumeRoleWithSAML,            STS returns temp creds
                                               POST assertion to SSO endpoint
* GetSessionToken (for MFA)

federation with SAML 2.0 is the old way, prefer SSO federation

SSO

when we need to login to:
* many AWS accounts
* many 3rd party business apps (Slack, Dropbox, Office 365, ...)
* many custom SAML applications
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
   Id store - [3rd IdP portal] - AssumeRoleWithSAML
              +- with SSO no need to manage all these portals, we connect directly to the Id store

Directory Services

Microsoft AD: centralized users/assets management from the domain controller

* managed Ms AD: on-prem <=> AWS - manage users on both (MFA supported)
* AD connector:  on-prem <=      - proxy to on-prem AD, manage all users in there
* simple AD:       N/A       AWS - AD-compatible, manage on AWS, no on-prem connection

Organizations

Root OU
   master account
      OU + member accounts

create accounts + OU (organizational units) per BU(business unit)/env/project/...

OU aren't accounts, they just help structuring the hierarchy

SCP (service control policies, restrictive by default)
* whitelist/blacklist IAM actions at the OU/account level
* does not affect service-linked roles

move account to another organization: delete from current, invite from 2nd

IAM

a role is both an identity and a 'resource' => it needs a trust policy to define who can assume the role
e.g lambdas have exe roles and resource-based policies (GUI bottom) which define the allowed callers

when you assume a role, you give up your original permissions!

an instance profile is a container for a single role that can be attached to an EC2 instance when launched

permission boundary (user, role, NOT group)
ex: if boundary = allow s3:*           on *
                  allow iam:CreateUser on * won't work
useful to restrict one specific user instead of a whole account with SCP

Service-Linked Role:
only a specific service can use this role vs a regular role which can be used by all services/users

principal - user, app, service
condition - aws:SourceIP, aws:RequestedRegion, ec2:ResourceTag
            "Bool" or "BoolIfExists" (MFA doesn't apply to all resources): {"aws:MultiFactorAuthPresent": false}

* arn:aws:s3:::my-bucket   => bucket level permission (e.g ListBucket)
* arn:awn:s3:::my-bucket/* => object level permission (e.g Get/PutObject)

Resource Access Manager

avoid resource duplication: share resources with any account

VPC subnets
* share within organization only
* network is shared => access via private IPs (cross-account SGs can be referenced but not viewed)

Encryption

KMS

share passwords/credentials/certificates/encryption at rest
data > 4KB => use envelope encryption
access: MANDATORY Key policy + optional IAM policy
                  +- default (complete access to root + allows access with IAM policies)

* AWS services use symmetric AES-256 CMK (customer master key) keys [create
  - AWS managed (free)                                               enable/disable
  - customer managed                                                 rotate]
    +- automatic rotation: once a year
    |  - same key id, new backing key (keep old one)
    +- manual rotation: if greater frequency needed or CMK is asymmetric so not eligible for automatic rotation
       - new key id, new backing key (keep old one)
       - apps use the key id so we need an alias to the id in this case
  - customer imported

* RSA, ECC (elliptic-curve cryptography) asymmetric keys are used for:
  - sign/verify integrity checks
  - outside of AWS (no access to KMS API)

A deleted CMK is in the 'pending deletion' status and can be recovered for 7 - 30days (default)

SSM Parameter Store

secure storage for configuration and secrets, version tracking
ssm.get_parameters(Names=['/site/prod/db-url'], WithDecryption=True)

std vs advanced (TTL in parameters policies, more params of bigger size + higher throughput)

AWS Secrets Manager (mostly for RDS MySQL, PostgreSQL, Aurora)

* rotation of secrets + new auto generation
* KMS encrypted

Shield

Route 53, CloudFront, Global Accelerator, ELB, EC2

WAF (web app firewall)

CloudFront, ALB, API Gateway

web ACLs:
* IP filtering
* http based rules (header, body, URI string)
* rate (DDoS) + geo-match rules
* SQL injection + XSS (cross-site scripting)

AWS Firewall Manager

common set of security rules at the organization level:
- WAF             (CloudFront, ALB,     API Gateway)
- Shield advanced (CloudFront, ALB/CLB, Elastic IP )
- SG              (EC2 + ENI)

GuardDuty (uses ML, anomaly detection, 3rd party data)

threat discovery (cryptocurrency attacks, malicious IPs):
* DNS logs
* VPC Flow logs
* CloudTrail mgmt events
* S3 data events

Amazon inspector

EC2 - agent     => OS vulnerabilities, CIS (center for internet security) benchmarks
      agentless => network accessibility
inspector service to send report via SNS

Amazon Macie

ML + pattern matching to alert about exposed (e.g in S3) sensitive data (PII: personally identifiable information)

VPC

IGW + routing table for the public subnets = Internet connectivity

* 5 VPC per region (soft limit)
* 5 CIDR per VPC: min /28 (    16 IPs)
                  max /16 (65 536 IPs)
* reserved addresses (e.g 10.0.0.0/24)
  - 10.0.0.0: network
  - 10.0.0.1: router
  - 10.0.0.2: DNS (or 169.254.169.253)
  - 10.0.0.3: future use
  - 10.0.0.255: broadcast (not supported in VPC!)

best practice:
           VPC /16 - 65 536
 public subnet /24 - 256 (we don't need too many hosts in a public subnet)
private subnet /20 - 4096

NACL (Network Access Control List aka subnet firewall)

* stateless: always needs in + out rules
* rules are evaluated from lowest to highest number, 1st one wins (low num = high precedence)
* good way of blocking a specific IP address at the subnet level
* default NACL => allow everything, new NACL => deny everything
* best practice: use increments of 100 to allow room for more rules

ephemeral ports

- clients connect to a defined port, and expect a response on an ephemeral port
- because NACL are stateless, we lose info about source port of incoming
  traffic, therefore outbound return traffic must go to all ephemeral ports:

  allow TCP/3306      to   db subnet  -->>  allow TCP/3306      from web subnet
  allow TCP/ephemeral from db subnet  <<--  allow TCP/ephemeral to   web subnet

DNS support/resolution (true by default):
it's best to have a DNS server within the VPC to avoid unnecessary network traffic,
route 53 (default) or custom

DNS hostnames (false by default, true for default VPC):
* needs enableDnsSupport=true
* if true => add public DNS for public instances

both needed for custom private DNS

DNS resolver

    AWS wants mitko.example.com? Route 53 resolver can forward queries to on-prem  via Route 53 outbound endpoint
on-prem wants       bla.aws.com? on-prem resolvers can forward queries to Route 53 via Route 53  inbound endpoint

In public subnet:

- bastion host => connect to private EC2 instances

  make it highly available:
  * ssh is layer 4 => multi-AZ NLB - ASG 1:1:1
  * bonus - thanks to the NLB, the bastion can be moved to the private subnet

- NAT (EC2) instance (deprecated) => Internet connectivity for private EC2 instances
  * disable source/destination check (can forward traffic)
  * must have elastic IP
  * private subnets to route via it

- NAT Gateway
  * elastic IP
  * single AZ (must create multiple NAT Gateways in multiple AZs for HA)
  * no SG to manage
  * 5 to 45 Gbps auto-scaling bandwidth
  * can't be used as bastion host

A private host behind NAT "can't" be contacted, for that you need NAT traversal:
Also known as UDP encapsulation, it allows traffic to get to the specified
destination which doesn't have a public IP address. In a S2S VPN connection, a
CGW behind NAT needs NAT-T enabled

VPC Peering: VPCs must not have overlapping CIDRs

VPC Endpoints (AWS PrivateLink)

* interface endpoints: ENI (private IP => SG)
* gateway endpoints (at no cost!): S3, DynamoDB

- connect to AWS services privately (from within your private subnets)
- DNS support must be on, route tables will need amending,
  no need for IGW or NATGW

VPC Endpoint Services (PrivateLink)

expose your own services (not AWS ones as above) through a NLB (or GWLB),
then consumers can connect via ENI thanks to PrivateLink

Flow Logs

troubleshoot SG & NACL issues

capture IP traffic
* VPC, subnets, ENI
* ELB, RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway... (managed interfaces)

 inbound accept (SG outbound accept), outbound reject => NACL
outbound accept (SG  inbound accept),  inbound reject => NACL

format:
ver | account | eni | src + dst IPs | src + dst ports | proto | packets | bytes | epoch start + end | action | status
                                                                                                      v   v
                                                                                                      SG, NACL - ACCEPT/REJECT

Traffic Mirroring

capture actual IP traffic for deeper inspection: tcpdump
e.g from EC2 ENI to another ENI or NLB

EC2-Classic (deprecated, CLB is another classic resource)

pre-VPC era: instances ran in a single network shared with other customers.
to link those old instances to our VPC, we need ClassicLink

Site-to-Site (S2S) VPN or DX

VPN:
IPsec over the public Internet

Direct Connect (DX):

* unencrypted private connection (add VPN between DX location and DC to have IPsec encryption)

* 1 month to setup connection
  - dedicated (1Gbps and 10Gbps)
  - hosted (capacity on-demand: 50Mbps, 500Mbps, 1, 2, 5 to 10Gbps)

* high resiliency:    multiple DX locations - multiple DCs
  maximum resiliency: multiple DX locations - multiple DCs
                       separate connections - separate connections
                       per location           per DC

    virtual private gateway - VPN or DX
   /
VGW                  <--> CGW (customer gateway)
                     <--> CGW (CloudHub hub-and-spoke model if multiple DCs)
     [ DX location ] <--> customer router in DC
       AWS|customer

VGW region 1 \
              <--> DX Gateway <-> DX <-> DC (direct connect for same region, direct connect gw cross-regions)
VGW region 2 /

AWS side
* must enable route propagation so subnets know how to contact the VPN gateway
* VPN concentrator (device that helps to manage multiple VPN connections => VPN on a larger scale)
* allows for custom ASN (Autonomous System Number). Edge location???

on-premises
* enable NAT-T if behind NAT

Transit Gateway (IP multicast)

transitive peering (traffic passes through) of multiple VPCs:
- peering
- VPNs
- DX Gateways

* share cross-account with RAM
* peer with other Transit Gateways across regions
* use route tables to limit communications

ECMP (equal-cost multi-path)

define multiple S2S VPN connections to increase the bandwidth of your connection to AWS

1x VPN gw = 2 tunnels = 1.25Gbps
2x                         5Gbps
3x                       7.5Gbps

IPv6

* IPv6 addresses are public and Internet-routable (no private range)
* egress-only IGW => same effect as a NAT gw but IPv6 are public so no NAT is needed
* 2001:db8::1234:5678 -> the middle 4 segments are zero
* IPv4 + IPv6 = dual-stack mode

Networking Costs in AWS

free for ingress traffic, we pay only when exiting AWS network

*  free  with private IPs within AZ
* $0.01  with private IPs  cross AZ
* $0.02  with  public IPs  cross AZ/region
* $0.09        out to S3  Internet (cross-region)
* $0.085 CloudFront + S3  Internet (actually cheaper and S3 requests are 7x cheaper => way better than previous row)

Disaster recovery

- RPO: Recovery Point Objective => minimize data loss
       /!\ disaster /!\
- RTO: Recovery  Time Objective => minimize downtime

on-premises to AWS cloud examples:
• Backup and restore                                  - backup/restore from snapshots:  cheapest ->   high RPO + RTO
• Pilot light  (bare core up in the cloud)            -                    DB replica:     cheap ->  lower RPO + RTO
• Warm standby (full min-size system up in the cloud) -        ELB + ASG + DB replica: expensive ->    low RPO + RTO
• Hot site     (full     size system up in the cloud) -        ELB + ASG + DB replica:    COSTLY -> lowest RPO + RTO
  multi site active-active approach

chaos: test your prod setup (ref. Netflix simian-army)

Migration to AWS

* VM import/export (VMs <-> EC2, or ami.iso to use on-premises)
* Migration Hub
* Application Discovery Service
*      Server Migration Service (SMS, live DMS-like migration)
*    Database Migration Service (section below)

DMS (database migration service)

source -- EC2 with DMS -- destination
+- all dbs                +- all dbs
+- S3                     +- S3
                          +- ElasticSearch
                          +- Kinesis data streams

* for heterogeneous migrations (different db engines), SCT (Schema ConversionTool) is needed beforehand.
* continuous data replication with CDC (change data capture): source remains available

DataSync

move large amounts of data to AWS (can be used together with snow family). storage gateways is for moving data to S3 only.

                                       => S3
on-prem NAS (NFS/SMB) + DataSync agent => EFS
                                       => FSx for Windows file server

         EFS + EC2 with DataSync agent => EFS (AWS to AWS)

AWS Backup

centralize AWS snapshots management:
* we need a plan (frequency + retention policy) and AWS services => it all goes to S3
* supports PITR (point in time recovery), tag-based backups, ...

HPC (high performance computing)

EC2 enhanced networking (SR-IOV): single root i/o virtualization:
single NIC to present itself as several virtual NICs
* ENA (elastic network adapter): higher PPS (packets per second) 100Gbps - or legacy Intel 82599 VF for up to 10Gbps
* EFA (elastic  fabric adapter): enhanced ENA leveraging MPI (message passing interface)
                                                         +- bypasses the underlying Linux OS for lower latency

AWS Batch: multi-node EC2/spot parallel jobs
AWS ParallelCluster: open source cluster management tool for HPC

CICD (continuous integration/delivery) CodePipeline

find/fix bugs early, deploy often
*                   push to CodeCommit - GitHub
*           build & test in CodeBuild  - Jenkins CI (continuous integration)
* deploy passing build with CodeDeploy - Jenkins CD (continuous delivery: create packages)
* provision with CloudFormation and/or Ansible (actual deploy???)

CloudFormation

IaC (infrastructure as code)
* YAML/JSON templates go in S3, deploy stack via cli
  - AWS resources
  - parameters: dynamic inputs
  - mappings:   static vars
  - outputs
  - conditionals
  - metadata
* figures out the right order of creation (declarative programming)
* estimate costs thanks to resource tags using the CloudFormation template
* dev env: save money by auto deleting 5pm / creating 8am templates
* auto diagrams

StackSets

Manage stacks across multiple accounts/regions with a single operation.
Update a stackset to update all stack instances.

EMR (elastic MapReduce): manage Apache Hadoop/Spark clusters to process/analyze big data
Opsworks: managed Chef & Puppet (alternative to AWS SSM)
WorkSpaces: VDI (Virtual Desktop Infrastructure), managed, secure cloud desktop (Linux/Windows)
AppSync: store and sync data across mobile and web apps in real-time (uses GraphQL from Facebook)
Cost Explorer: Savings Plan alternative to Reserved Instances
Transcribe: ASR (automatic speech recognition) service => convert audio to text

Well Architected Framework 5 Pillars

* Operational excellence: IaC, anticipate failure
  CloudFormation, AWS Config, monitoring, CICD

* Security
  IAM, security at all levels, encryption, keep people away from data

* Reliability
  stop guessing capacity => ASG, test/automate recovery

* Performance efficiency
  use serverless, stay up-to-date: AWS News Blog

* Cost optimization
  Cost Explorer, Trusted Advisor, spot instances
                 |
                 • cost optimization
                 • performance
                 • security
                 • faulttolerance
                 • service limits/quotas

https://aws.amazon.com/architecture/
https://aws.amazon.com/solutions/

Misc

1 CPU = multiple cores + multiple threads. vCPU is the total of threads.

http statefullness can be achieved with:
* ELB stickiness (session/client affinity)
* cookies stored on EC2 instances or sent by user (web cookies)
* single session_id cookie sent by client, session info stored in ElastiCache

NOT serverless (you have to provision the EC2 instance/node type):
* RDS
* Aurora (can be)
* Redshift
* ElastiCache
* EMR

* while exploring metrics in the console, I couldn't filter for sqs, it only worked by filtering for aws first! bug?
* spot fleet, no auto scaling capabilities?
